This page provides instructions on how to configure Claude Code with LLM gateway solutions.[1] LLM gateways act as a centralized proxy between Claude Code and model providers, enabling features like centralized authentication, usage tracking, cost controls, audit logging, and model routing.[1]

The document specifically details configuration with LiteLLM, a third-party proxy service.[1] It outlines prerequisites, basic setup, and various authentication methods, including static API keys and dynamic keys using a helper script.[1] The guide recommends using a unified endpoint for benefits like load balancing and consistent tracking.[1] It also provides alternative configurations for provider-specific pass-through endpoints for Anthropic API, Amazon Bedrock, and Google Vertex AI.[1] Finally, it touches on model selection and provides links to additional resources.[1]

Sources:
[1] LLM gateway configuration - Anthropic (https://docs.anthropic.com/en/docs/claude-code/llm-gateway)